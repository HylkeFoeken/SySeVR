Environment:
joern 0.3.1, neo4j 2.1.5, python 3.6, tensorflow 1.6, gensim 3.4

help files:
dict_cwe2father.pkl - maps CWE child to parents 
example: (CWE-121 -> CWE-787, CWE-788)

label_vec_type.pkl - list of CWE-types for vulnerability type vector for each slice
example: ['CWE-128', 'CWE-131', 'CWE-135', ...]

dict_testcase2code.pkl - maps SARD test case file and line number to vulnerable lines of code
example: '74218/CWE124_Buffer_Underwrite__CWE839_rand_45.c/36' --> u'            buffer[data] = 1;'

dict_flawline2filepath.pkl - maps SARD test case files to typed vulnerable lines 
example: '74218/CWE124_Buffer_Underwrite__CWE839_rand_45.c' --> [{u'36': u'CWE-124'}]

Step 1: Generating slices (i.e., SeVCs)
(1) Use joern to parse source code: the input is source code files, and the output is a file named .joernIndex.

(2) get_cfg_relation.py: This file is used to get CFG graphs of functions using joern tool. The input is output of the first step, and the outputs are stored with folders in cfg_db. 

(3) complete_PDG.py: This file is used to get PDG graph of functions. The inputs are files in cfg_db, and the outputs are stored with folders in pdg_db.

(4) access_db_operate.py: This file is used to get the call graph of functions. The inputs are files in pdg_db, and the outputs are stored with folders in dict_call2cfgNodeID_funcID.

(5) points_get.py: This file is used to get four kinds of SyVCs. The inputs are files in dict_call2cfgNodeID_funcID, and the outputs are four kinds of SyVCs.
(sensifunc_slice_points.pkl, pointuse_slice_points.pkl, arrayuse_slice_points.pkl, integeroverflow_slice_points_new.pkl)

(6) extract_df.py: This file is used to extract slices. The inputs are files generated by points_get.py, and the outputs are slice files.
(test_data/4/api_slices.txt, test_data/4/pointersuse_slices.txt, test_data/4/arraysuse_slices.txt, test_data/4/integeroverflow_slices.txt)
nb. SyVCs are filtered by CWE-type, does this exclude the SARD samples?

(7) make_label.py: This file is used to get labels of slices.
input: dict_flawline2filepath.pkl, slices\api_slices.txt, slices\arraysuse_slices.txt, slices\pointersuse_slices.txt, slices\integeroverflow_slices.txt
output:
 labels\api_slices.pkl, vulnlines\api_slices.pkl,
 labels\array_slic.pkl, vulnlines\array_slice.pkl,
 labels\pointer_slice.pkl, vulnlines\pointer_slice.pkl,
 labels\expr_slice.pkl, vulnlines\expr_slice.pkl

(8) data_preprocess.py: This file is used to write the labels to the slice files.
input: 
output: api_slices.txt, arraysuse_slices.txt, integeroverflow_slices.txt, pointersuse_slices.txt

Step 2: Data preprocess

(1) process_dataflow_func.py: This file is used to process the slices, including read the pkl file and split codes into corpus.
The inputs are the slice file and the label file, and the output is the corpus file named with testcase id.
input: SySeVR/Implementation/source2slice//test_data/4/*.txt
output: SySeVR/Implementation/source2slice/test_data/4/corpus/test_data/api_slices.pkl
(2) create_w2vmodel.py: This file is used to train word2vec model. The inputs are corpus files, and the output is the word2vec model.
input: SySeVR/Implementation/source2slice/test_data/4/corpus/test_data/api_slices.pkl
output: SySeVR/Implementation/source2slice/test_data/4/w2v_model/wordmodel3

(3) get_dl_input.py: This file is used to convert tokens of slices in corpus files into vectors by trained word2vec model.
The input is the trained word2vec model and corpus files, and the outputs are vector files.
input:
- CORPUSPATH = "../source2slice/test_data/4/corpus/"
- VECTORPATH = "../source2slice/test_data/4/vector/"
- W2VPATH = "../source2slice/test_data/4/w2v_model/wordmodel3"

output:
- SySeVR/Implementation/source2slice/test_data/4/dl_input/train/
- SySeVR/Implementation/source2slice/test_data/4/dl_input/test/

(4) dealrawdata.py: This file is used to make vectors generated by get_dl_input.py into fixed length. 
input:
- raw_traindataSetPath = "../source2slice/test_data/4/dl_input/train/"
- raw_testdataSetPath = "../source2slice/test_data/4/dl_input/test/"

output:
- traindataSetPath = "../source2slice/test_data/4/dl_input_shuffle/train/"
- testdataSetPath = "../source2slice/test_data/4/dl_input_shuffle/test/"

Step 3: Deep Learning Model

(1) bgru.py: This file is used to train BGRU model and get test results. The inputs are vector files generated by dealrawdata.py, and the output is trained model and test results.
input:
output:
(2) preprocess_dl_Input_version5.py: This file is used to preprocess data imported into model. It is imported by bgru.py.
input:
output: